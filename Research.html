<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="ja" http-equiv="Content-Language">

<link href="./css/index.css" rel="stylesheet" type="text/css">
<style type="text/css">
.auto-style1 {
	text-align: center;
}
</style>
<title>Sudipta N. Sinha: Research</title>
</head>

<body>
<div id="menu-box">
<ul>
<li><a href="./index.html">Home</a></li>
<li><a href="./Bio.html">Bio</a></li>
<li><a href="./Publications.html">Publications</a></li>
<li><a href="./Research.html">Research</a></li>
<li><a href="./Activities.html">Activities</a></li>
<li><a href="./Others.html">Others</a></li>
</ul>
</div>

<div class="space">&nbsp;</div>
<div id="body-box">

<br>
<h2>Sudipta N. Sinha - Research</h2>


<table width="100%" border="0" cellspacing="0" cellpadding="0">

<tbody>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/Do-etal-CVPR22_300w.png" width="130"></td>
<td style="width: 80%">
<span class="textStyle">
<strong>
Learning to Detect Scene Landmarks for Camera Localization</strong><br>
<a href="http://cvpr2022.thecvf.com/">CVPR 2022</a><br>
<font size = 2>
We present a new learned camera localization technique that eliminates the need to store features or a detailed 3D point cloud. Our key idea is to implicitly encode the appearance of a sparse yet salient set of 3D scene points into a convolutional neural network (CNN) that can detect these scene points in query images whenever they are visible.
<a href = "https://github.com/microsoft/SceneLandmarkLocalization">...</a>
</font><br>
<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://github.com/microsoft/SceneLandmarkLocalization"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://www.youtube.com/watch?v=HM2yLCLz5nY"><img src="img/yt.jpg" width="24"></a><br>

</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SpecialeICCV2019.jpg" width="130"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Privacy Preserving Image Queries for Camera Localization</strong><br>
<a href="http://iccv2019.thecvf.com/">ICCV 2019</a><br>
<font size = 2>
We have developed a new 6-DoF camera localization technique that conceals the content of the query image when localization is performed in a cloud-based service. This is a follow up of our previous work on privacy preserving camera localization where we devised a way to conceal the 3D point cloud map which is used for localization.
<a href = "https://www.microsoft.com/en-us/research/project/privacy-preserving-image-queries-for-camera-localization/">...</a>
</font><br>
<a href="pdfs/SpecialeICCV2019.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://www.microsoft.com/en-us/research/project/privacy-preserving-image-queries-for-camera-localization/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SpecialeCVPR2019.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Privacy Preserving Image-Based Localization</strong><br>
<a href="http://cvpr2019.thecvf.com/">CVPR 2019</a><br>
<font size = 2>
How can we avoid disclosing confidential information about the captured 3D scene, and yet allow reliable camera pose estimation? This paper proposes the first solution to what we call privacy preserving image-based localization using new ideas from geometry.
<a href = "https://www.microsoft.com/en-us/research/blog/envisioning-privacy-preserving-image-based-localization-for-augmented-reality/">...</a>
</font><br>
<a href="pdfs/SpecialeCVPR2019.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://www.microsoft.com/en-us/research/blog/envisioning-privacy-preserving-image-based-localization-for-augmented-reality/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/PittalugaCVPR2019.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Inverting Structure from Motion Reconstructions</strong><br>
<a href="http://cvpr2019.thecvf.com/">CVPR 2019</a><br>
<font size = 2>
We show, for the first time, that SfM point clouds and features retain enough information to reveal scene appearance and compromise privacy. We present a privacy attack that reconstructs color images of the scene.
<a href = "
https://www.francescopittaluga.com/invsfm/index.html">...</a>
</font><br>
<a href="pdfs/SpecialeCVPR2019.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://www.francescopittaluga.com/invsfm/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://www.youtube.com/watch?v=M4U9sXE84rY"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SchoenbergerECCV2018.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Learning to Fuse Proposals in Semi-Global Matching
</strong><br>
<a href="https://eccv2018.org">ECCV 2018</a><br>
<font size = 2>
We propose SGM-Forest, an efficient extension to the SGM stereo matching algorithm, that uses a random decision forest classifier to fuse multiple disparity map proposals, each of which is obtained by solving an independent 1D scanline optimization problem. SGM-Forest is consistently more accurate than SGM and its performance generalize very well to new datasets.
<a href = "https://www.microsoft.com/en-us/research/publication/learning-to-fuse-proposals-from-multiple-scanline-optimizations-in-semi-global-matching/">...</a>
</font><br>
<a href="pdfs/SchoenbergerECCV2018.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/learning-to-fuse-proposals-from-multiple-scanline-optimizations-in-semi-global-matching/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2018.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Real-Time Seamless Single Shot 6D Object Pose Prediction</strong><br>
<a href="http://cvpr2018.thecvf.com/">CVPR 2018</a><br>
<font size = 2>
We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task (Kehl et al. ICCV'17) that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing <a href = "proj/SSS6DObjectPose/index.html">...</a>
</font><br>
<a href="pdfs/TekinCVPR2018.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://github.com/Microsoft/singleshotpose"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ICCV2017.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Submodular Trajectory Optimization for Aerial 3D Scanning</strong><br>
<a href="http://iccv2017.thecvf.com/">ICCV 2017</a><br>
<font size = 2>
Drones equipped with cameras are emerging as a powerful tool for large-scale aerial 3D scanning, but existing automatic flight planners do not exploit all available information about the scene, and can therefore produce inaccurate and incomplete 3D models. We present an automatic method to generate drone trajectories, such that the imagery acquired during the flight will later produce a high-fidelity 3D model. Our method uses a coarse estimate of the scene geometry to plan camera trajectories that: (1) cover the scene as thoroughly as possible; (2) encourage observations of scene geometry from a diverse set of viewing angles; (3) avoid obstacles; and (4) respect a user-specified flight time budget <a href = "http://graphics.stanford.edu/papers/aerial_scanning/">...</a>
</font><br>
<a href="pdfs/RobertsICCV2017.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://graphics.stanford.edu/papers/aerial_scanning/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2017.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>Fast Multi-frame Stereo Scene Flow with Motion Segmentation</strong><br>
<a href="http://cvpr2017.thecvf.com/">CVPR 2017</a><br>
<font size = 2>
We propose a new multi-frame method for efficiently computing scene flow (dense depth and optical flow) and camera ego-motion for a dynamic scene observed from a moving stereo camera rig. Our technique also segments out moving objects from the rigid scene. In our method, we first estimate the disparity map and the 6-DOF camera motion using stereo matching and visual odometry. We then identify regions inconsistent with the estimated camera motion and compute per-pixel optical flow <a href = "">...</a>
</font><br>
<a href="pdfs/TaniaiCVPR2017.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://taniai.space/projects/cvpr17_fsf/"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://www.youtube.com/watch?v=hkj3sVaC6jg"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2017b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Flight Dynamics-based Recovery of a UAV Trajectory using Ground Cameras</strong><br>
<a href="http://cvpr2017.thecvf.com/">CVPR 2017</a><br>
<font size = 2>
We propose a new method to estimate the 6-dof trajectory of a flying object such as a quadrotor UAV within a 3D airspace monitored using multiple fixed ground cameras. It is based on a new structure from motion formulation for the 3D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics) <a href = "">...</a>
</font><br>
<a href="pdfs/RozantsevCVPR2017.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://snsinha.github.io/proj/UAVTrajRecon/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://www.youtube.com/watch?v=udZdh0VsxvM"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/NSDI2017.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
FarmBeats: An IoT Platform for Data-Driven Agriculture</strong><br>
<a href="https://www.usenix.org/conference/nsdi17/glance">NSDI 2017</a><br>
<font size = 2>

Data-driven techniques help boost agricultural productivity by increasing yields, reducing losses and cutting down input costs. However, these techniques have seen sparse adoption owing to high costs of manual data collection and limited connectivity solutions. In this paper, we present FarmBeats, an end-to-end IoT platform for agriculture that enables seamless data collection from various sensors, cameras and drones. FarmBeats’s system design that explicitly accounts for weather-related power and Internet outages <a href = "pdfs/VasishtNSDI2017.pdf">...</a>
</font><br>
<a href="pdfs/VasishtNSDI2017.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="p"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/TPAMI2017.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Robust Multiview Photometric Stereo using Planar Mesh Parameterization</strong><br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI 2017</a><br>
<font size = 2>
We propose a robust uncalibrated multiview photometric stereo method for high quality 3D shape reconstruction. In our method, a coarse initial 3D mesh obtained using a multiview stereo method is projected onto a 2D planar domain using a planar mesh parameterization technique. We describe methods for surface normal estimation that work in the parameterized 2D space that jointly incorporates all geometric and photometric cues from multiple<a href = "http://jaesik.info/publications/multiviewps/index.html">...</a>
</font><br>
<a href="http://jaesik.info/publications/multiviewps/PAMI2016.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://jaesik.info/publications/multiviewps/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/JG2Zq1Ot-oY"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/TPAMI2017b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Multiview Rectification of Folded Documents</strong><br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI 2017</a><br>
<font size = 2>
Digitally unwrapping images of paper sheets is crucial for accurate document scanning and text recognition. This paper presents a method for automatically rectifying curved or folded paper sheets from a few images captured from multiple viewpoints. Prior methods either need expensive 3D scanners or model deformable surfaces using over-simplified parametric representations. In contrast, our method uses regular images and is based on general developable surface models<a href = "https://dl.acm.org/citation.cfm?id=3186773">...</a>
</font><br>
<a href="https://dl.acm.org/citation.cfm?id=3186773"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="https://dl.acm.org/citation.cfm?id=3186773"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>

<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2016a.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Efficient and Robust Color Consistency for Community Photo Collections</strong><br>
<a href="p">CVPR 2016</a><br>
<font size = 2>
We present an efficient technique to optimize color consistency of a collection of images depicting a common scene. Our method first recovers sparse pixel correspondences in the input images and stacks them into a matrix with many missing entries. We show that this matrix satisfies a rank two constraint under a simple color correction model. These parameters can be viewed as pseudo white balance and gamma correction parameters for each input image. We present a robust low-rank matrix factorization method<a href = "http://jaesik.info/publications/photoconsistency/index.html">...</a>
</font><br>
<a href="http://jaesik.info/publications/photoconsistency/paper.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://jaesik.info/publications/photoconsistency/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/nnH-CJaRxas"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2016b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Joint Recovery of Dense Correspondence and Cosegmentation in Two Images</strong><br>
<a href="http://cvpr2016.thecvf.com/">CVPR 2016</a><br>
<font size = 2>
We propose a new technique to jointly recover cosegmentation and dense per-pixel correspondence in two images. Our method parameterizes the correspondence ﬁeld using piecewise similarity transformations and recovers a mapping between the estimated common “foreground” regions in the two images allowing them to be precisely aligned. Our formulation is based on a hierarchical Markov random ﬁeld model with segmentation and transformation labels. The hierarchical structure uses nested image regions to constrain inference<a href = "http://taniai.space/projects/cvpr16_dccs/">...</a>
</font><br>

<a href="pdfs/TaniaiCVPR2016.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://taniai.space/projects/cvpr16_dccs/cvpr2016-dccs-final-long.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://taniai.space/projects/cvpr16_dccs/"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/Jt23XOp1Fow"><img src="img/yt.jpg" width="24"></a><br>

</span>
</td>
</tr>




<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ICRA2015.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Monocular Localization of a moving person onboard a Quadrotor MAV</strong><br>
<a href="www.icra2015.org/">ICRA 2015</a><br>
<font size = 2>
In this paper, we propose a novel method to recover the 3D trajectory of a moving person from a monocular camera mounted on a quadrotor micro aerial vehicle (MAV). The key contribution is an integrated approach that simultaneously performs visual odometry (VO) and persistent tracking of a person automatically detected in the scene. All computation pertaining to VO, detection and tracking runs onboard the MAV from a front-facing monocular RGB camera<a href = "proj\MavLoc\index.html">...</a>
</font><br>
<a href="pdfs/LimICRA2015.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj\MavLoc\index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/YTU6ro3jguY"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2014a.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Calibrating a non-isotropic near point light source using a plane</strong><br>
<a href="p">CVPR 2014</a><br>
<font size = 2>
We show that a non-isotropic near point light source rigidly attached to a camera can be calibrated using multiple images of a weakly textured planar scene. Weprovethat if the radiant intensity distribution (RID) of a light source is radially symmetric with respect to its dominant direction, then the shading observed on a Lambertian scene plane is bilaterally symmetric with respect to a 2D line on the plane<a href = "http://jaesik.info/publications/lightcalib/">...</a>
</font><br>
<a href="pdfs/ParkCVPR2014.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://jaesik.info/publications/lightcalib/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2014b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
High-Resolution Stereo Matching using Local Plane Sweeps</strong><br>
<a href="http://www.pamitc.org/cvpr14/">CVPR 2014</a><br>
<font size = 2>
We present a stereo algorithm designed for speed and efficiency that uses local slanted plane sweeps to propose disparity hypotheses for a semi-global matching algorithm. Our local plane hypotheses are derived from initial sparse feature correspondences followed by an iterative clustering step. Local plane sweeps are then performed around each slanted plane to produce out-of-plane parallax and matching-cost estimates<a href = "proj/LPS/index.html">...</a>
</font><br>
<a href="pdfs/SinhaCVPR2014.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/LPS/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SpinPS2.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
3D Spin Movies and Photosynth 2</strong><br>
<a href="https://www.microsoft.com/en-us/research/blog/new-spin-photosynth">2013</a><br>
<font size = 2>
We propose a way to create more realistic transitions when moving from photograph to photograph, thereby providing an immersive viewing experience. Such transitions were made possible using computer-vision techniques to calculate the depth of each pixel for all the images in the collection.
<a href = "https://www.microsoft.com/en-us/research/blog/new-spin-photosynth">...</a>
</font><br>
<a href="https://www.microsoft.com/en-us/research/blog/new-spin-photosynth/"><img src="img/www.jpg" width="24"></a>&nbsp;
<a href="https://www.youtube.com/watch?v=_WjMwbmJj8o"><img src="img/yt.jpg" width="24"></a><br>
<a href="https://www.youtube.com/watch?v=e1m_4jpqoq8"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ICCV2013.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Multiview Photometric Stereo using Planar Mesh Parameterization</strong><br>
<a href="http://www.pamitc.org/iccv13/">ICCV 2013</a><br>
<font size = 2>
We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is ﬁrst parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly <a href = "http://jaesik.info/publications/multiviewps/index.html">...</a>
</font><br>
<a href="pdfs/ParkICCV2013.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://jaesik.info/publications/multiviewps/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/JG2Zq1Ot-oY"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2013.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</strong><br>
<a href="http://www.pamitc.org/cvpr13/">CVPR 2013</a><br>
<font size = 2>
In this paper we propose a new technique for learning a discriminative codebook for local feature descriptors, specifically designed for scalable landmark classification. The key contribution lies in exploiting the knowledge of correspondences
within sets of feature descriptors during codebook learning. Feature correspondences are obtained using structure from motion (SfM) computation on Internet photo collections <a href = "proj/LandmarkCL/index.html">...</a>
</font><br>
<a href="pdfs/BergamoCVPR2013.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/LandmarkCL/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ECCV2012a.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Detecting and Reconstructing 3D Mirror Symmetric Objects</strong><br>
<a href="http://eccv2012.unifi.it/">ECCV 2012</a><br>
<font size = 2>
We present a system that detects 3D mirror-symmetric objects in images and then reconstructs their visible symmetric parts. Our detection stage is based on matching mirror symmetric feature points and descriptors and then estimating the symmetry direction using RANSAC. We enhance this step by augmenting feature descriptors with their affine deformed versions and matching these extended sets of descriptors<a href = "http://jaesik.info/publications/multiviewps/index.html">...</a>
</font><br>
<a href="pdfs/SinhaECCV2012.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/SymmRecon3D/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>



<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ECCV2012b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Multiple View Object Cosegmentation using Appearance and Stereo Cues</strong><br>
<a href="http://eccv2012.unifi.it/">ECCV 2012</a><br>
<font size = 2>
We present an automatic approach to segment an object in calibrated images acquired from multiple viewpoints. Our system starts with a new piecewise planar layer-based stereo algorithm that estimates a dense depth map that consists of a set of 3D planar surfaces. The algorithm is formulated using an energy minimization framework that combines stereo and appearance cues, where for each surface<a href = "proj/MVSeg/index.html">...</a>
</font><br>
<a href="pdfs/KowdleECCV2012.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/MVSeg/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2012a.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Real-time Image-based 6-DOF Localization in Large-Scale Environments</strong><br>
<a href="https://sites.google.com/a/cvpr2012.org/home/">CVPR 2012</a><br>
<font size = 2>
We present a real-time approach for image-based localization within large scenes that have been reconstructed offline using structure from motion (Sfm). From monocular video, our method continuously computes a precise 6-DOF camera pose, by efficiently tracking natural features and matching them to 3D points in the Sfm point cloud. Our main contribution lies in efficiently interleaving a fast keypoint tracker that uses inexpensive binary feature descriptors with a new approach for direct 2D-to-3D matching<a href = "proj/ImgLoc/index.html">...</a>
</font><br>
<a href="pdfs/LimCVPR2012.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/ImgLoc/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="proj/ImgLoc/supp/limCVPR12-supp.mov"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2012b.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Discovering and Exploiting 3D Symmetries in Structure from Motion
</strong><br>
<a href="https://sites.google.com/a/cvpr2012.org/home/">CVPR 2012</a><br>
<font size = 2>
Many architectural scenes contain symmetric or repeated structures, which can generate erroneous image correspondences during structure from motion (Sfm) computation. Prior work has shown that the detection and removal of these incorrect matches is crucial for accurate and robust recovery of scene structure. In this paper, we point out that these incorrect matches, in fact, provide strong cues to the existence of symmetries and structural regularities in the unknown 3D structure<a href = "http://www.cvg.ethz.ch/research/symmetries-in-sfm/">...</a>
</font><br>
<a href="pdfs/CohenCVPR2012.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://www.cvg.ethz.ch/research/symmetries-in-sfm/"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tbody><tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SIG2012.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Image-Based Rendering for Scenes with Reflections</strong><br>
<a href="http://s2012.siggraph.org/">SIGGRAPH 2012</a><br>
<font size = 2>
We present a system for image-based modeling and rendering of real-world scenes containing reflective and glossy surfaces. Previous approaches to image-based rendering assume that the scene can be approximated by 3D proxies that enable view interpolation using traditional back-to-front or z-buffer compositing. In this work, we show how these can be generalized to multiple layers that are combined in an additive fashion to model the reflection and transmission of light<a href = "proj\ReflectionsIBR\index.html">...</a>
</font><br>
<a href="pdfs/SinhaSIG2012.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj\ReflectionsIBR\index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/mRYtKJqcAU0"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/CVPR2011.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Structure from motion for scenes with large duplicate structures</strong><br>
<a href="http://www.cvpapers.com/cvpr2011.html">CVPR 2011</a><br>
<font size = 2>
Most existing structure from motion (SFM) approaches for unordered images cannot handle multiple instances of the same structure in the scene. When image pairs containing different instances are matched based on visual similarity, the pairwise geometric relations as well as the correspondences inferred from such pairs are erroneous, which can lead to catastrophic failures in the reconstruction<a href = "proj/DupSFM/index.html">...</a>
</font><br>
<a href="pdfs/RobertsCVPR2011.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/DupSFM/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ECCVw2010.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
A Multi-Stage Linear Approach to Structure from Motion</strong><br>
<a href="https://graphics.cs.ucy.ac.cy/RMLE/">ECCV 2010 workshop</a><br>
<font size = 2>
 We present a new structure from motion (Sfm) technique based on point and vanishing point (VP) matches in images. First, all global camera rotations are computed from VP matches as well as relative rotation estimates obtained from pairwise image matches. A new multi-staged linear technique is then used to estimate all camera translations and 3D points simultaneously<a href = "proj/LinearSFM/index.html">...</a>
</font><br>
<a href="pdfs/SinhaECCV-W2010.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/LinearSFM/index.html"><img src="img/www.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/ICCV2009.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Piecewise Planar Stereo for Image-based Rendering</strong><br>
<a href="http://www.cvpapers.com/iccv2009.html">ICCV 2009</a><br>
<font size = 2>
We present a novel multi-view stereo method designed for image-based rendering that generates piecewise planar depth maps from an unordered collection of photographs. First a discrete set of 3D plane candidates are computed based on a sparse point cloud of the scene (recovered by structure from motion) and sparse 3D line segments reconstructed from multiple views<a href = "proj/PlanarStereo/index.html">...</a>
</font><br>
<a href="pdfs/SinhaICCV2009.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="proj/PlanarStereo/index.html"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="proj/PlanarStereo/video1589.avi"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>


<tr class="spaceUnder">
<td style="width: 20%">
<img alt="" src="img/SIGAsia2008.jpg" width="150"></td>
<td style="width: 80%">			
<span class="textStyle">
<strong>
Interactive 3D Architectural Modeling from Unordered Photo Collections</strong><br>
<a href="https://www.siggraph.org//asia2008/">SIGGRAPH Asia 2008</a><br>
<font size = 2>
We present an interactive system for generating photorealistic, textured, piecewise-planar 3D models of architectural structures and urban scenes from unordered sets of photographs. To reconstruct 3D geometry in our system, the user draws outlines overlaid on 2D photographs. The 3D structure is then automatically computed by combining the 2D interaction with the multi-view geometric information recovered by performing structure from motion analysis on the input photographs<a href = "http://vis.berkeley.edu/papers/photoModel/">...</a>
</font><br>
<a href="pdfs/SinhaSIGAsia2008.pdf"><img src="img/pdf.jpg" width="24"></a>&nbsp;<a href="http://vis.berkeley.edu/papers/photoModel/"><img src="img/www.jpg" width="24"></a>&nbsp;<a href="https://youtu.be/RhNNLxHwz6s"><img src="img/yt.jpg" width="24"></a>&nbsp;<a href="http://vis.berkeley.edu/papers/photoModel/SinhaSIGASIA08.suppvideo.avi"><img src="img/yt.jpg" width="24"></a><br>
</span>
</td>
</tr>



</tbody></table>

<div id="footer-box">
<hr>
Last updated: April 19, 2018
</div>

</body></html>